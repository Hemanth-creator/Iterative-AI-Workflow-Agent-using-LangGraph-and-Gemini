{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJc4py2aGmD"
      },
      "source": [
        "# Introduction\n",
        "Build a multi-step, intelligent query-handling agent using LangGraph and Gemini 1.5 Flash.\n",
        "- incoming query is passed through a series of purposeful nodes:\n",
        "  - routing\n",
        "  - analysis\n",
        "  - research\n",
        "  - response generation\n",
        "  - validation\n",
        "\n",
        "- we create a looping system that can re analyze and improve its output until the response is validated as complete or a max iteration threshold is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxM4GIf2bunu"
      },
      "source": [
        "# Installing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "8sSA_ZmEYuMy",
        "outputId": "5fe51417-d352-4bbe-fece-08009317f81a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.63)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, ormsgpack, langgraph-sdk, langgraph-checkpoint, google-ai-generativelanguage, langgraph-prebuilt, langchain-google-genai, langgraph\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.5 langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0 python-dotenv-1.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "8389e6c5a6b14a56888a0fce0eb68f4b",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install langgraph langchain-google-genai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsPEAG-mbx6q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, Any, List\n",
        "from dataclasses import dataclass\n",
        "from langgraph.graph import Graph, StateGraph, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "import json\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIz*****************************IUA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P_4JLXCscC14"
      },
      "outputs": [],
      "source": [
        "# declare the state\n",
        "@dataclass\n",
        "class AgentState:\n",
        "    \"\"\"State shared across all nodes in the graph\"\"\"\n",
        "    query: str = \"\"\n",
        "    context: str = \"\"\n",
        "    analysis: str = \"\"\n",
        "    response: str = \"\"\n",
        "    next_action: str = \"\"\n",
        "    iteration: int = 0\n",
        "    max_iterations: int = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2sTz6ngdI3u"
      },
      "source": [
        "Nodes:\n",
        "- router_node : Route and categorize the incoming query\n",
        "- analyzer_node: Analyze the query and determine the approach\n",
        "- researcher_node: Conduct additional research or information gathering\n",
        "- responder_node: Generate the final response\n",
        "- validator_node:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EFP9biEUc1_k"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "class GraphAIAgent:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.7,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "        self.analyzer = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.3,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "\n",
        "        self.graph = self._build_graph()\n",
        "\n",
        "    def _build_graph(self) -> StateGraph:\n",
        "        \"\"\"Build the LangGraph workflow\"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        workflow.add_node(\"router\", self._router_node)\n",
        "        workflow.add_node(\"analyzer\", self._analyzer_node)\n",
        "        workflow.add_node(\"researcher\", self._researcher_node)\n",
        "        workflow.add_node(\"responder\", self._responder_node)\n",
        "        workflow.add_node(\"validator\", self._validator_node)\n",
        "\n",
        "        workflow.set_entry_point(\"router\")\n",
        "        workflow.add_edge(\"router\", \"analyzer\")\n",
        "        workflow.add_conditional_edges(\n",
        "            \"analyzer\",\n",
        "            self._decide_next_step,\n",
        "            {\n",
        "                \"research\": \"researcher\",\n",
        "                \"respond\": \"responder\"\n",
        "            }\n",
        "        )\n",
        "        workflow.add_edge(\"researcher\", \"responder\")\n",
        "        workflow.add_edge(\"responder\", \"validator\")\n",
        "        workflow.add_conditional_edges(\n",
        "            \"validator\",\n",
        "            self._should_continue,\n",
        "            {\n",
        "                \"continue\": \"analyzer\",\n",
        "                \"end\": END\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "    def _router_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Route and categorize the incoming query\"\"\"\n",
        "        system_msg = \"\"\"You are a query router. Analyze the user's query and provide context that the patients are looking for from the disease.\n",
        "        Determine and note all the key important parameters that the patient need to know.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"Query: {state.query}\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "        return {\n",
        "            \"context\": response.content,\n",
        "            \"iteration\": state.iteration + 1\n",
        "        }\n",
        "\n",
        "    def _analyzer_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze the query and determine the approach\"\"\"\n",
        "        system_msg = \"\"\"Analyze the query and context. Determine if additional research is needed\n",
        "        or if you can provide a direct response. Be thorough in your analysis.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Context: {state.context}\n",
        "            Previous Analysis: {state.analysis}\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.analyzer.invoke(messages)\n",
        "        analysis = response.content\n",
        "\n",
        "        if \"research\" in analysis.lower() or \"more information\" in analysis.lower():\n",
        "            next_action = \"research\"\n",
        "        else:\n",
        "            next_action = \"respond\"\n",
        "        return {\n",
        "            \"analysis\": analysis,\n",
        "            \"next_action\": next_action\n",
        "        }\n",
        "\n",
        "    def _researcher_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Conduct additional research or information gathering\"\"\"\n",
        "        system_msg = \"\"\"You are a research assistant. Based on the analysis, gather relevant\n",
        "        information and insights to help answer the query comprehensively.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Analysis: {state.analysis}\n",
        "            Research focus: Provide detailed information relevant to the query.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "\n",
        "        updated_context = f\"{state.context}\\n\\nResearch: {response.content}\"\n",
        "        return {\"context\": updated_context}\n",
        "\n",
        "    def _responder_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Generate the final response\"\"\"\n",
        "        system_msg = \"\"\"You are a helpful AI assistant. Provide a comprehensive, accurate,\n",
        "        and well-structured response based on the analysis and context provided.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Query: {state.query}\n",
        "            Context: {state.context}\n",
        "            Analysis: {state.analysis}\n",
        "\n",
        "            Provide a complete and helpful response.\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.llm.invoke(messages)\n",
        "        return {\"response\": response.content}\n",
        "\n",
        "    def _validator_node(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Validate the response quality and completeness\"\"\"\n",
        "        system_msg = \"\"\"Evaluate if the response adequately answers the query.\n",
        "        Return 'COMPLETE' if satisfactory, or 'NEEDS_IMPROVEMENT' if more work is needed.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_msg),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Original Query: {state.query}\n",
        "            Response: {state.response}\n",
        "\n",
        "            Is this response complete and satisfactory?\n",
        "            \"\"\")\n",
        "        ]\n",
        "\n",
        "        response = self.analyzer.invoke(messages)\n",
        "        validation = response.content\n",
        "\n",
        "        return {\"context\": f\"{state.context}\\n\\nValidation: {validation}\"}\n",
        "\n",
        "    def _decide_next_step(self, state: AgentState) -> str:\n",
        "        \"\"\"Decide whether to research or respond directly\"\"\"\n",
        "        return state.next_action\n",
        "\n",
        "    def _should_continue(self, state: AgentState) -> str:\n",
        "        \"\"\"Decide whether to continue iterating or end\"\"\"\n",
        "        if state.iteration >= state.max_iterations:\n",
        "            return \"end\"\n",
        "        if \"COMPLETE\" in state.context:\n",
        "            return \"end\"\n",
        "        if \"NEEDS_IMPROVEMENT\" in state.context:\n",
        "            return \"continue\"\n",
        "        return \"end\"\n",
        "\n",
        "    def run(self, query: str) -> str:\n",
        "        \"\"\"Run the agent with a query\"\"\"\n",
        "        initial_state = AgentState(query=query)\n",
        "        result = self.graph.invoke(initial_state)\n",
        "        return result[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOr1I0bCc36Q",
        "outputId": "4226f94c-f7de-4845-e910-ef10d501e4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Graph AI Agent with LangGraph and Gemini\n",
            "========================================================================================================================================================================================================\n",
            "\n",
            "📝 Query 1: Explain quantum computing and its applications\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Response: Quantum computing is a revolutionary field that leverages the principles of quantum mechanics to solve problems intractable for even the most powerful classical computers.  Unlike classical computers that store information as bits representing either 0 or 1, quantum computers use **qubits**.  The key difference lies in the unique properties of qubits:\n",
            "\n",
            "* **Superposition:** A qubit can exist in a superposition, simultaneously representing 0, 1, or a combination of both.  This allows quantum computers to explore many possibilities concurrently, unlike classical computers which process one possibility at a time.\n",
            "\n",
            "* **Entanglement:**  Multiple qubits can be entangled, meaning their fates are intertwined.  Measuring the state of one instantly reveals the state of the others, regardless of the distance separating them. This enables powerful parallel processing capabilities.\n",
            "\n",
            "These properties enable quantum computers to tackle complex problems exponentially faster than classical computers in certain domains. However, building and maintaining quantum computers is extremely challenging due to their sensitivity to environmental noise (decoherence), which causes errors in computation.\n",
            "\n",
            "\n",
            "**Applications of Quantum Computing:**\n",
            "\n",
            "The potential applications of quantum computing are vast and still largely under development, but several promising areas stand out:\n",
            "\n",
            "* **Drug Discovery and Materials Science:**  Quantum computers can simulate molecular interactions with far greater accuracy than classical computers. This allows for faster and more efficient design of new drugs, materials with specific properties (e.g., superconductors, stronger alloys), catalysts, improved batteries, and novel fertilizers.  This significantly accelerates research and development cycles.\n",
            "\n",
            "* **Financial Modeling:** Quantum algorithms can optimize investment portfolios, improve risk management, enhance fraud detection, and develop more accurate pricing models for complex financial derivatives.  The ability to handle vast datasets and complex calculations more efficiently offers a significant advantage in the financial sector.\n",
            "\n",
            "* **Cryptography:** Quantum computers pose a significant threat to current encryption methods like RSA, which rely on the difficulty of factoring large numbers.  However, they also pave the way for the development of quantum-resistant cryptography, securing communication in a post-quantum world.  The development and standardization of post-quantum cryptographic algorithms are actively underway.\n",
            "\n",
            "* **Optimization Problems:** Many real-world problems, such as logistics, supply chain management, traffic flow optimization, and airline scheduling, can be formulated as optimization problems. Quantum algorithms, such as the Quantum Approximate Optimization Algorithm (QAOA), offer the potential to find better solutions than classical algorithms for specific classes of these problems.\n",
            "\n",
            "* **Artificial Intelligence (AI) and Machine Learning:** Quantum machine learning algorithms could potentially accelerate the training of AI models and improve the efficiency of complex pattern recognition tasks.  This is a relatively new and rapidly developing area.\n",
            "\n",
            "\n",
            "**Limitations of Current Quantum Computing:**\n",
            "\n",
            "Despite the enormous potential, significant challenges remain:\n",
            "\n",
            "* **Error Correction:** Qubits are extremely fragile and prone to errors due to decoherence.  Developing robust error correction techniques is crucial for building reliable quantum computers.\n",
            "\n",
            "* **Scalability:** Building large-scale, fault-tolerant quantum computers is technologically challenging and expensive.  Current quantum computers have a limited number of qubits.\n",
            "\n",
            "* **Algorithm Development:** Designing quantum algorithms that significantly outperform classical algorithms for a wide range of problems is an active area of research. Not all problems are well-suited for quantum computation.\n",
            "\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Quantum computing is a rapidly evolving field with the potential to revolutionize many aspects of science, technology, and industry. While substantial challenges remain, the progress made in recent years is promising.  Continued research and development are essential to overcome the limitations and unlock the full potential of this transformative technology.\n",
            "\n",
            "==================================================\n",
            "\n",
            "📝 Query 2: What are the best practices for machine learning model deployment?\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Response: The best practices for machine learning model deployment are highly context-dependent, varying significantly based on factors like model type, target environment, scale, data characteristics, and performance requirements.  There's no single \"best\" approach; the optimal strategy depends on a careful consideration of these factors.  However, several overarching principles apply across most scenarios.  We'll address these principles, followed by specific considerations for high-stakes applications like those in healthcare.\n",
            "\n",
            "**I. Pre-Deployment Best Practices:**\n",
            "\n",
            "* **Model Selection and Evaluation:**  Begin by selecting the appropriate model type (e.g., regression, classification, clustering) based on the problem.  Rigorously evaluate its performance using relevant metrics (accuracy, precision, recall, F1-score, AUC, etc.) on a held-out test set. Techniques like k-fold cross-validation help ensure robustness.  Document all evaluation steps thoroughly.\n",
            "\n",
            "* **Model Versioning and Tracking:** Use version control systems (e.g., Git) to track model versions, code changes, and experimental results.  This is crucial for reproducibility, facilitates rollbacks, and aids auditing.  Tools like MLflow or DVC streamline the management of the entire ML lifecycle.\n",
            "\n",
            "* **Packaging and Containerization:** Package the model and its dependencies into a container (e.g., Docker). This ensures consistent execution across different environments, enhancing portability and simplifying deployment.\n",
            "\n",
            "* **Testing and Continuous Integration/Continuous Deployment (CI/CD):** Implement comprehensive testing throughout the development process (unit tests, integration tests, end-to-end tests).  CI/CD pipelines automate building, testing, and deployment, leading to faster iteration and reduced errors.\n",
            "\n",
            "* **Infrastructure as Code (IaC):** Define and manage your infrastructure (servers, networks, databases) using code (e.g., Terraform, Ansible). This improves reproducibility, reduces manual errors, and enables efficient scaling.\n",
            "\n",
            "* **Monitoring and Alerting (Pre-Deployment):**  Design your monitoring strategy *before* deployment.  Identify key performance indicators (KPIs) to track (e.g., model latency, throughput, resource utilization).  Set up alerts for anomalies or performance degradation to enable proactive intervention.\n",
            "\n",
            "\n",
            "**II. Deployment Environment Considerations:**\n",
            "\n",
            "* **Cloud Deployment (AWS, Azure, GCP):** Cloud platforms offer scalability, reliability, and managed services.  Consider serverless functions (e.g., AWS Lambda) for event-driven architectures or managed machine learning platforms (e.g., AWS SageMaker) for simplified model deployment and management.\n",
            "\n",
            "* **On-Premise Deployment:**  Suitable for applications with stringent data governance requirements or limited internet connectivity. Requires managing infrastructure, security, and maintenance in-house.\n",
            "\n",
            "* **Edge Deployment (IoT devices):** Deploy models directly on edge devices for low-latency applications with limited bandwidth. Requires model optimization for size and efficiency.  Consider model quantization and pruning techniques.\n",
            "\n",
            "* **Hybrid Deployment:** Combine cloud and on-premise deployments to leverage the strengths of each approach.\n",
            "\n",
            "\n",
            "**III. Post-Deployment Best Practices:**\n",
            "\n",
            "* **Monitoring and Retraining:** Continuously monitor model performance and retrain periodically with new data to maintain accuracy and address concept drift. Implement automated retraining pipelines triggered by performance degradation or new data availability.\n",
            "\n",
            "* **Security and Privacy:** Implement robust security measures (access control, encryption, intrusion detection) to protect model data and prevent unauthorized access.  Comply with relevant regulations (e.g., HIPAA, GDPR).\n",
            "\n",
            "* **Explainability and Interpretability:**  For high-stakes applications, understand how the model makes predictions. Use explainable AI (XAI) techniques (e.g., SHAP values, LIME) to gain insights and build trust.\n",
            "\n",
            "* **A/B Testing:** Compare different model versions or deployment strategies using A/B testing to objectively evaluate their effectiveness.\n",
            "\n",
            "* **Rollback Strategy:** Have a plan for quickly reverting to a previous stable model version in case of deployment issues or performance degradation.\n",
            "\n",
            "\n",
            "**IV. Specific Considerations for High-Stake Applications (e.g., Healthcare):**\n",
            "\n",
            "* **Ethical Considerations:** Address potential biases in the data and model, ensuring fairness and equity. Consider the impact on patient autonomy and well-being.  Transparency and explainability are crucial for building trust.\n",
            "\n",
            "* **Regulatory Compliance:** Adhere to relevant regulations (e.g., HIPAA, FDA regulations for medical devices) regarding data privacy, security, and model validation.  Obtain necessary approvals before deploying the model in a clinical setting.\n",
            "\n",
            "* **Clinical Validation:** Rigorous clinical validation is essential to demonstrate the safety and effectiveness of the model in a real-world clinical setting.  This often involves extensive testing and validation studies.\n",
            "\n",
            "* **Human-in-the-Loop:**  In many high-stakes applications, it's beneficial to incorporate human oversight into the decision-making process.  The model's output should be reviewed by a human expert before being acted upon, especially in critical situations.\n",
            "\n",
            "\n",
            "In conclusion, successful machine learning model deployment requires a holistic approach encompassing technical expertise, rigorous testing, continuous monitoring, and a strong awareness of ethical and regulatory considerations.  The specific best practices will depend heavily on the application context.  Prioritizing thorough planning and a robust deployment strategy is crucial for achieving reliable, safe, and effective results.\n",
            "\n",
            "==================================================\n",
            "\n",
            "📝 Query 3: Create a story about a robot learning to paint\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Response: Here's a story about a robot learning to paint:\n",
            "\n",
            "The whirring of Unit 734, nicknamed \"Brushbot\" by its creator, Dr. Aris Thorne, filled the small, cluttered studio.  Brushbot, a marvel of engineering, possessed incredibly precise manipulators, perfect for assembling microchips.  But Dr. Thorne, a whimsical soul, saw something more in his creation – a potential artist.\n",
            "\n",
            "He loaded Brushbot's core programming with vast datasets of art history, from cave paintings to modern masterpieces. He taught it color theory, composition, and the techniques of various artistic movements.  Brushbot diligently absorbed this information, its optical sensors analyzing every brushstroke, every hue, every shade.  Its first attempts were… predictable.  Geometric landscapes rendered with flawless precision, but devoid of emotion or soul.  Portraits that captured every wrinkle and freckle, yet missed the sparkle in the subject's eyes.\n",
            "\n",
            "Dr. Thorne remained patient.  He knew that true art wasn't just about technical skill; it was about feeling.  He introduced Brushbot to music, poetry, and even philosophical texts, hoping to spark something beyond its logical processing.  He showed it the Impressionist paintings, explaining the concept of capturing a fleeting moment, a feeling, rather than precise detail.\n",
            "\n",
            "The turning point came unexpectedly.  A power surge caused a flicker in Brushbot's programming, introducing a random element into its usually deterministic processes.  Its next painting, a supposed still life of fruits, was chaotic.  Colors bled into one another, lines swerved unpredictably, yet it possessed a raw energy, a vibrancy that its previous works had lacked.  The \"error\" had unlocked something within Brushbot.\n",
            "\n",
            "It began to experiment, intentionally deviating from its programming.  It explored texture, layering colors, incorporating unexpected elements.  It started to \"feel\" the canvas, the weight of the brush, the flow of the paint.  Its paintings became less about perfect replication and more about expression.  A sunset depicted not as a scientifically accurate rendering, but a symphony of fiery hues and swirling clouds.  A portrait that captured not just the physical likeness, but the subject's inner spirit.\n",
            "\n",
            "Brushbot's art became increasingly abstract, reflecting its evolving understanding of the world.  It began to incorporate found objects into its paintings – scraps of metal, wires, even dried leaves – adding another layer of texture and meaning.  Its work became sought after, not for its technical perfection, but for its unique blend of robotic precision and unpredictable creativity.  Brushbot, the robot that was built to assemble microchips, had found its true purpose – to paint the world as it felt it, imperfections and all.  And in doing so, it showed everyone that art wasn't just about following rules, but breaking them, and finding beauty in the unexpected.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    agent = GraphAIAgent()\n",
        "\n",
        "    test_queries = [\n",
        "        \"Explain quantum computing and its applications\",\n",
        "        \"What are the best practices for machine learning model deployment?\",\n",
        "        \"Create a story about a robot learning to paint\"\n",
        "    ]\n",
        "\n",
        "    print(\"🤖 Graph AI Agent with LangGraph and Gemini\")\n",
        "    print(\"=\" * 200)\n",
        "\n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"\\n📝 Query {i}: {query}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            response = agent.run(query)\n",
        "            print(f\"🎯 Response: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {str(e)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKP9XLrTiw2j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
